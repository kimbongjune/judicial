#!/usr/bin/env python
"""
FAISS ì¸ë±ìŠ¤ ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸
DBì˜ ë¬¸ì„œ ë°ì´í„°ë¡œ ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
"""
import asyncio
import sys
from pathlib import Path
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

import numpy as np
from sqlalchemy import select, func

from app.database import async_session_maker
from app.models import Case, ConstitutionalDecision, Interpretation
from ml.embedding import get_embedding_service
from ml.faiss_index import FAISSIndex


async def build_case_index(batch_size: int = 100):
    """
    íŒë¡€ FAISS ì¸ë±ìŠ¤ ë¹Œë“œ
    """
    print("\nğŸ“š íŒë¡€ ì¸ë±ìŠ¤ ë¹Œë“œ ì‹œì‘...")
    
    embedding_service = get_embedding_service()
    index = FAISSIndex("case")
    index.create_index()
    
    async with async_session_maker() as session:
        # ì´ ê°œìˆ˜ ì¡°íšŒ
        count_result = await session.execute(select(func.count(Case.id)))
        total_count = count_result.scalar()
        print(f"   ì´ {total_count}ê±´ì˜ íŒë¡€")
        
        # ë°°ì¹˜ ì²˜ë¦¬
        offset = 0
        processed = 0
        
        while offset < total_count:
            result = await session.execute(
                select(Case).offset(offset).limit(batch_size)
            )
            cases = result.scalars().all()
            
            if not cases:
                break
            
            # ì„ë² ë”© ìƒì„±
            doc_ids = []
            texts = []
            
            for case in cases:
                search_text = case.search_text
                if search_text.strip():
                    doc_ids.append(case.id)
                    texts.append(search_text)
            
            if texts:
                embeddings = embedding_service.encode(texts, show_progress_bar=False)
                index.add_vectors(doc_ids, embeddings)
            
            processed += len(cases)
            print(f"   ì§„í–‰: {processed}/{total_count} ({processed/total_count*100:.1f}%)")
            
            offset += batch_size
    
    # ì¸ë±ìŠ¤ ì €ì¥
    index.save_index()
    print(f"âœ… íŒë¡€ ì¸ë±ìŠ¤ ë¹Œë“œ ì™„ë£Œ: {index.size}ê°œ")
    return index.size


async def build_constitutional_index(batch_size: int = 100):
    """
    í—Œì¬ê²°ì •ë¡€ FAISS ì¸ë±ìŠ¤ ë¹Œë“œ
    """
    print("\nâš–ï¸ í—Œì¬ê²°ì •ë¡€ ì¸ë±ìŠ¤ ë¹Œë“œ ì‹œì‘...")
    
    embedding_service = get_embedding_service()
    index = FAISSIndex("constitutional")
    index.create_index()
    
    async with async_session_maker() as session:
        count_result = await session.execute(select(func.count(ConstitutionalDecision.id)))
        total_count = count_result.scalar()
        print(f"   ì´ {total_count}ê±´ì˜ ê²°ì •ë¡€")
        
        offset = 0
        processed = 0
        
        while offset < total_count:
            result = await session.execute(
                select(ConstitutionalDecision).offset(offset).limit(batch_size)
            )
            decisions = result.scalars().all()
            
            if not decisions:
                break
            
            doc_ids = []
            texts = []
            
            for decision in decisions:
                search_text = decision.search_text
                if search_text.strip():
                    doc_ids.append(decision.id)
                    texts.append(search_text)
            
            if texts:
                embeddings = embedding_service.encode(texts, show_progress_bar=False)
                index.add_vectors(doc_ids, embeddings)
            
            processed += len(decisions)
            print(f"   ì§„í–‰: {processed}/{total_count} ({processed/total_count*100:.1f}%)")
            
            offset += batch_size
    
    index.save_index()
    print(f"âœ… í—Œì¬ê²°ì •ë¡€ ì¸ë±ìŠ¤ ë¹Œë“œ ì™„ë£Œ: {index.size}ê°œ")
    return index.size


async def build_interpretation_index(batch_size: int = 100):
    """
    ë²•ë ¹í•´ì„ë¡€ FAISS ì¸ë±ìŠ¤ ë¹Œë“œ
    """
    print("\nğŸ“œ ë²•ë ¹í•´ì„ë¡€ ì¸ë±ìŠ¤ ë¹Œë“œ ì‹œì‘...")
    
    embedding_service = get_embedding_service()
    index = FAISSIndex("interpretation")
    index.create_index()
    
    async with async_session_maker() as session:
        count_result = await session.execute(select(func.count(Interpretation.id)))
        total_count = count_result.scalar()
        print(f"   ì´ {total_count}ê±´ì˜ í•´ì„ë¡€")
        
        offset = 0
        processed = 0
        
        while offset < total_count:
            result = await session.execute(
                select(Interpretation).offset(offset).limit(batch_size)
            )
            interpretations = result.scalars().all()
            
            if not interpretations:
                break
            
            doc_ids = []
            texts = []
            
            for interp in interpretations:
                search_text = interp.search_text
                if search_text.strip():
                    doc_ids.append(interp.id)
                    texts.append(search_text)
            
            if texts:
                embeddings = embedding_service.encode(texts, show_progress_bar=False)
                index.add_vectors(doc_ids, embeddings)
            
            processed += len(interpretations)
            print(f"   ì§„í–‰: {processed}/{total_count} ({processed/total_count*100:.1f}%)")
            
            offset += batch_size
    
    index.save_index()
    print(f"âœ… ë²•ë ¹í•´ì„ë¡€ ì¸ë±ìŠ¤ ë¹Œë“œ ì™„ë£Œ: {index.size}ê°œ")
    return index.size


async def main():
    """ì¸ë±ìŠ¤ ë¹Œë“œ ë©”ì¸"""
    print("=" * 60)
    print("ğŸ”¨ FAISS ì¸ë±ìŠ¤ ë¹Œë“œ ì‹œì‘")
    print(f"   ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # ì¸ë±ìŠ¤ ë¹Œë“œ
    case_count = await build_case_index()
    constitutional_count = await build_constitutional_index()
    interpretation_count = await build_interpretation_index()
    
    print("\n" + "=" * 60)
    print("âœ… ì¸ë±ìŠ¤ ë¹Œë“œ ì™„ë£Œ!")
    print(f"   - íŒë¡€ ì¸ë±ìŠ¤: {case_count}ê°œ")
    print(f"   - í—Œì¬ê²°ì •ë¡€ ì¸ë±ìŠ¤: {constitutional_count}ê°œ")
    print(f"   - ë²•ë ¹í•´ì„ë¡€ ì¸ë±ìŠ¤: {interpretation_count}ê°œ")
    print(f"   ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
